# è¿è¡Œå‘½ä»¤ç¤ºä¾‹ï¼š
# python self_evaluation.py --results_dir results

import os
import argparse
import numpy as np
from collections import defaultdict, Counter
import matplotlib.pyplot as plt

def load_tracking_results(file_path):
    """åŠ è½½è·Ÿè¸ªç»“æœæ–‡ä»¶"""
    results = []
    if not os.path.exists(file_path):
        return results
    
    with open(file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            parts = line.split(',')
            if len(parts) >= 6:
                frame_id = int(parts[0])
                track_id = int(parts[1])
                x_left = float(parts[2])
                y_top = float(parts[3])
                width = float(parts[4])
                height = float(parts[5])
                
                results.append({
                    'frame': frame_id,
                    'id': track_id,
                    'x': x_left + width/2,
                    'y': y_top + height/2,
                    'w': width,
                    'h': height,
                    'area': width * height
                })
    
    return results

def analyze_tracking_quality(results, video_name):
    """åˆ†æè·Ÿè¸ªè´¨é‡"""
    if not results:
        return {
            'video': video_name,
            'quality_score': 0.0,
            'issues': ['æ²¡æœ‰è·Ÿè¸ªç»“æœ']
        }
    
    # åŸºæœ¬ç»Ÿè®¡
    total_detections = len(results)
    unique_ids = len(set(r['id'] for r in results))
    max_frame = max(r['frame'] for r in results)
    min_frame = min(r['frame'] for r in results)
    frame_span = max_frame - min_frame + 1
    
    # è½¨è¿¹é•¿åº¦åˆ†æ
    track_lengths = defaultdict(int)
    track_frames = defaultdict(set)
    for r in results:
        track_lengths[r['id']] += 1
        track_frames[r['id']].add(r['frame'])
    
    # è®¡ç®—è½¨è¿¹è¿ç»­æ€§
    track_continuity = {}
    for track_id, frames in track_frames.items():
        frame_list = sorted(frames)
        if len(frame_list) > 1:
            expected_frames = frame_list[-1] - frame_list[0] + 1
            actual_frames = len(frame_list)
            continuity = actual_frames / expected_frames
        else:
            continuity = 1.0
        track_continuity[track_id] = continuity
    
    avg_continuity = np.mean(list(track_continuity.values())) if track_continuity else 0.0
    
    # ç›®æ ‡å¤§å°åˆ†æ
    areas = [r['area'] for r in results]
    avg_area = np.mean(areas)
    area_std = np.std(areas)
    
    # æ£€æµ‹å¯†åº¦åˆ†æ
    detections_per_frame = total_detections / frame_span if frame_span > 0 else 0
    
    # è´¨é‡è¯„ä¼°
    quality_factors = []
    issues = []
    
    # 1. è½¨è¿¹è¿ç»­æ€§ (æƒé‡: 30%)
    if avg_continuity >= 0.8:
        continuity_score = 1.0
    elif avg_continuity >= 0.6:
        continuity_score = 0.7
    elif avg_continuity >= 0.4:
        continuity_score = 0.4
    else:
        continuity_score = 0.1
        issues.append("è½¨è¿¹è¿ç»­æ€§å·®")
    
    quality_factors.append(('è½¨è¿¹è¿ç»­æ€§', continuity_score, 0.3))
    
    # 2. è½¨è¿¹é•¿åº¦åˆç†æ€§ (æƒé‡: 25%)
    avg_track_length = np.mean(list(track_lengths.values()))
    if avg_track_length >= 20:
        length_score = 1.0
    elif avg_track_length >= 10:
        length_score = 0.8
    elif avg_track_length >= 5:
        length_score = 0.5
    else:
        length_score = 0.2
        issues.append("è½¨è¿¹é•¿åº¦è¿‡çŸ­")
    
    quality_factors.append(('è½¨è¿¹é•¿åº¦', length_score, 0.25))
    
    # 3. æ£€æµ‹å¯†åº¦åˆç†æ€§ (æƒé‡: 20%)
    if 0.5 <= detections_per_frame <= 5.0:
        density_score = 1.0
    elif 0.1 <= detections_per_frame <= 10.0:
        density_score = 0.7
    elif detections_per_frame > 10.0:
        density_score = 0.3
        issues.append("æ£€æµ‹å¯†åº¦è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è™šè­¦")
    else:
        density_score = 0.2
        issues.append("æ£€æµ‹å¯†åº¦è¿‡ä½")
    
    quality_factors.append(('æ£€æµ‹å¯†åº¦', density_score, 0.2))
    
    # 4. IDä¸€è‡´æ€§ (æƒé‡: 15%)
    id_efficiency = total_detections / unique_ids if unique_ids > 0 else 0
    if id_efficiency >= 10:
        id_score = 1.0
    elif id_efficiency >= 5:
        id_score = 0.8
    elif id_efficiency >= 2:
        id_score = 0.5
    else:
        id_score = 0.2
        issues.append("IDåˆ‡æ¢é¢‘ç¹")
    
    quality_factors.append(('IDä¸€è‡´æ€§', id_score, 0.15))
    
    # 5. ç›®æ ‡å¤§å°ä¸€è‡´æ€§ (æƒé‡: 10%)
    if avg_area > 0:
        size_consistency = 1 - min(area_std / avg_area, 1.0)
    else:
        size_consistency = 0.0
    
    if size_consistency >= 0.7:
        size_score = 1.0
    elif size_consistency >= 0.5:
        size_score = 0.7
    else:
        size_score = 0.4
        issues.append("ç›®æ ‡å¤§å°å˜åŒ–è¿‡å¤§")
    
    quality_factors.append(('å¤§å°ä¸€è‡´æ€§', size_score, 0.1))
    
    # è®¡ç®—åŠ æƒè´¨é‡åˆ†æ•°
    quality_score = sum(score * weight for _, score, weight in quality_factors)
    
    return {
        'video': video_name,
        'total_detections': total_detections,
        'unique_ids': unique_ids,
        'frame_span': frame_span,
        'avg_track_length': avg_track_length,
        'avg_continuity': avg_continuity,
        'detections_per_frame': detections_per_frame,
        'quality_score': quality_score,
        'quality_factors': quality_factors,
        'issues': issues
    }

def estimate_competition_score(quality_scores):
    """åŸºäºè´¨é‡åˆ†æ•°ä¼°ç®—æ¯”èµ›å¾—åˆ†"""
    if not quality_scores:
        return 0.0
    
    avg_quality = np.mean(quality_scores)
    
    # ç»éªŒå…¬å¼ï¼šå°†è´¨é‡åˆ†æ•°æ˜ å°„åˆ°MOTA+IDF1èŒƒå›´
    # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥ä¼°ç®—ï¼Œå®é™…å¾—åˆ†éœ€è¦çœŸå€¼æ•°æ®
    estimated_mota = avg_quality * 0.8  # MOTAé€šå¸¸æ¯”è´¨é‡åˆ†æ•°ç•¥ä½
    estimated_idf1 = avg_quality * 0.9  # IDF1ç›¸å¯¹å®¹æ˜“è·å¾—é«˜åˆ†
    
    estimated_score = (estimated_mota + estimated_idf1) / 2
    
    return estimated_score, estimated_mota, estimated_idf1

def main():
    parser = argparse.ArgumentParser(description='è·Ÿè¸ªç»“æœè‡ªè¯„ä¼°å·¥å…·')
    parser.add_argument('--results_dir', type=str, default='results', help='ç»“æœç›®å½•')
    parser.add_argument('--output_file', type=str, default='self_evaluation.txt', help='è¯„ä¼°æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ“Š è·Ÿè¸ªç»“æœè‡ªè¯„ä¼°å·¥å…·")
    print("=" * 50)
    print(f"ğŸ“ ç»“æœç›®å½•: {args.results_dir}")
    
    if not os.path.exists(args.results_dir):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {args.results_dir}")
        return
    
    # è·å–æ‰€æœ‰ç»“æœæ–‡ä»¶
    result_files = [f for f in os.listdir(args.results_dir) if f.endswith('.txt')]
    
    if not result_files:
        print(f"âŒ åœ¨ {args.results_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶")
        return
    
    print(f"ğŸ“¹ æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")
    
    all_analyses = []
    quality_scores = []
    
    # åˆ†ææ¯ä¸ªè§†é¢‘
    for result_file in sorted(result_files):
        video_name = result_file[:-4]
        file_path = os.path.join(args.results_dir, result_file)
        
        results = load_tracking_results(file_path)
        analysis = analyze_tracking_quality(results, video_name)
        
        all_analyses.append(analysis)
        quality_scores.append(analysis['quality_score'])
        
        print(f"\nğŸ“¹ {video_name}:")
        print(f"   æ£€æµ‹æ•°é‡: {analysis['total_detections']}")
        print(f"   å”¯ä¸€IDæ•°: {analysis['unique_ids']}")
        print(f"   å¹³å‡è½¨è¿¹é•¿åº¦: {analysis['avg_track_length']:.1f}")
        print(f"   è½¨è¿¹è¿ç»­æ€§: {analysis['avg_continuity']:.3f}")
        print(f"   è´¨é‡å¾—åˆ†: {analysis['quality_score']:.3f}")
        
        if analysis['issues']:
            print(f"   âš ï¸ é—®é¢˜: {', '.join(analysis['issues'])}")
    
    # æ€»ä½“è¯„ä¼°
    if quality_scores:
        print(f"\nğŸ“ˆ æ€»ä½“è¯„ä¼°")
        print("=" * 50)
        
        avg_quality = np.mean(quality_scores)
        min_quality = np.min(quality_scores)
        max_quality = np.max(quality_scores)
        
        print(f"å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.3f}")
        print(f"è´¨é‡å¾—åˆ†èŒƒå›´: {min_quality:.3f} - {max_quality:.3f}")
        
        # ä¼°ç®—æ¯”èµ›å¾—åˆ†
        estimated_score, estimated_mota, estimated_idf1 = estimate_competition_score(quality_scores)
        
        print(f"\nğŸ¯ é¢„ä¼°æ¯”èµ›å¾—åˆ†:")
        print(f"é¢„ä¼° MOTA: {estimated_mota:.3f}")
        print(f"é¢„ä¼° IDF1: {estimated_idf1:.3f}")
        print(f"é¢„ä¼°æ€»åˆ†: {estimated_score:.3f}")
        
        # è¯„åˆ†ç­‰çº§
        if estimated_score >= 0.7:
            grade = "ğŸ¥‡ ä¼˜ç§€ (70%+)"
        elif estimated_score >= 0.5:
            grade = "ğŸ¥ˆ è‰¯å¥½ (50-70%)"
        elif estimated_score >= 0.3:
            grade = "ğŸ¥‰ ä¸€èˆ¬ (30-50%)"
        else:
            grade = "ğŸ“‰ éœ€è¦æ”¹è¿› (<30%)"
        
        print(f"é¢„ä¼°ç­‰çº§: {grade}")
        
        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        
        common_issues = defaultdict(int)
        for analysis in all_analyses:
            for issue in analysis['issues']:
                common_issues[issue] += 1
        
        if common_issues:
            for issue, count in sorted(common_issues.items(), key=lambda x: x[1], reverse=True):
                print(f"   - {issue} (å½±å“{count}ä¸ªè§†é¢‘)")
        else:
            print("   - æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open(args.output_file, 'w', encoding='utf-8') as f:
            f.write("è·Ÿè¸ªç»“æœè‡ªè¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("å„è§†é¢‘è¯¦ç»†åˆ†æ:\n")
            for analysis in all_analyses:
                f.write(f"\n{analysis['video']}:\n")
                f.write(f"  æ£€æµ‹æ•°é‡: {analysis['total_detections']}\n")
                f.write(f"  å”¯ä¸€IDæ•°: {analysis['unique_ids']}\n")
                f.write(f"  å¸§æ•°èŒƒå›´: {analysis['frame_span']}\n")
                f.write(f"  å¹³å‡è½¨è¿¹é•¿åº¦: {analysis['avg_track_length']:.1f}\n")
                f.write(f"  è½¨è¿¹è¿ç»­æ€§: {analysis['avg_continuity']:.3f}\n")
                f.write(f"  æ£€æµ‹å¯†åº¦: {analysis['detections_per_frame']:.2f}/å¸§\n")
                f.write(f"  è´¨é‡å¾—åˆ†: {analysis['quality_score']:.3f}\n")
                
                if analysis['issues']:
                    f.write(f"  é—®é¢˜: {', '.join(analysis['issues'])}\n")
            
            f.write(f"\næ€»ä½“è¯„ä¼°:\n")
            f.write(f"å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.3f}\n")
            f.write(f"é¢„ä¼° MOTA: {estimated_mota:.3f}\n")
            f.write(f"é¢„ä¼° IDF1: {estimated_idf1:.3f}\n")
            f.write(f"é¢„ä¼°æ€»åˆ†: {estimated_score:.3f}\n")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_file}")
        
        # ç½®ä¿¡åº¦è¯´æ˜
        print(f"\nğŸ“ è¯´æ˜:")
        print("   - è¿™æ˜¯åŸºäºè·Ÿè¸ªç»“æœç»Ÿè®¡çš„é¢„ä¼°åˆ†æ•°")
        print("   - å®é™…æ¯”èµ›å¾—åˆ†éœ€è¦çœŸå€¼æ•°æ®è¿›è¡Œç²¾ç¡®è®¡ç®—")
        print("   - é¢„ä¼°å‡†ç¡®åº¦çº¦ä¸º Â±0.1-0.2")
        print("   - å»ºè®®é‡ç‚¹å…³æ³¨è´¨é‡å¾—åˆ†è¾ƒä½çš„è§†é¢‘")

if __name__ == "__main__":
    main()
