#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡å‡†æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
å‚è€ƒå·¥ä¸šç•Œæ ‡å‡†æµ‹é€Ÿæ–¹æ³•ï¼Œæä¾›å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°
åŒ…å«ï¼š1.æ¨¡å‹éªŒè¯ 2.TorchScriptæµ‹è¯• 3.ONNXæµ‹è¯• 4.ä¸“ä¸šé€Ÿåº¦æµ‹è¯•
"""

import warnings
warnings.filterwarnings('ignore')

import argparse
import time
import numpy as np
import torch
import os
import sys
from pathlib import Path
from tqdm import tqdm
from ultralytics import YOLO
from ultralytics.utils.torch_utils import select_device

def get_weight_size(path):
    """è·å–æ¨¡å‹æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
    stats = os.stat(path)
    return f'{stats.st_size / 1024 / 1024:.1f}'

def check_onnx_gpu_support():
    """æ£€æŸ¥ONNX Runtime GPUæ”¯æŒ"""
    try:
        import onnxruntime as ort

        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„onnxruntime
        try:
            available_providers = ort.get_available_providers()
        except AttributeError:
            # æ—§ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰è¿™ä¸ªæ–¹æ³•
            try:
                # å°è¯•åˆ›å»ºä¸€ä¸ªä¼šè¯æ¥æ£€æŸ¥CUDAæ”¯æŒ
                import numpy as np
                dummy_model = b'\x08\x01\x12\x0c\x08\x01\x12\x08\x08\x01\x12\x04\x08\x01\x10\x01'  # æœ€å°çš„ONNXæ¨¡å‹
                session = ort.InferenceSession(dummy_model, providers=['CUDAExecutionProvider'])
                available_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            except:
                available_providers = ['CPUExecutionProvider']

        cuda_available = 'CUDAExecutionProvider' in available_providers

        print(f"ğŸ“‹ ONNX Runtime å¯ç”¨æä¾›è€…: {available_providers}")
        if cuda_available:
            print("âœ… ONNX Runtime æ”¯æŒ CUDA GPU åŠ é€Ÿ")
        else:
            print("âš ï¸  ONNX Runtime ä¸æ”¯æŒ CUDAï¼Œå°†ä½¿ç”¨ CPU")
            print("ğŸ’¡ å®‰è£…GPUç‰ˆæœ¬: pip install onnxruntime-gpu")

        return cuda_available
    except ImportError:
        print("âŒ ONNX Runtime æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âš ï¸  æ£€æŸ¥ONNX Runtimeæ—¶å‡ºé”™: {e}")
        print("ğŸ”„ å‡è®¾æ”¯æŒCPUï¼Œç»§ç»­æµ‹è¯•...")
        return False

def setup_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")

    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ï¼Œé¿å…è¿æ¥é”™è¯¯
    try:
        torch.multiprocessing.set_start_method('spawn', force=True)
        print("âœ… å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•è®¾ç½®æˆåŠŸ")
    except RuntimeError as e:
        print(f"âš ï¸  å¤šè¿›ç¨‹è®¾ç½®è­¦å‘Š: {e}")

    # æ£€æŸ¥ONNX Runtime GPUæ”¯æŒ
    print("\nğŸ” æ£€æŸ¥ONNX Runtime GPUæ”¯æŒ...")
    check_onnx_gpu_support()

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = "last.pt"
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨")
        sys.exit(1)

    # æ£€æŸ¥æ•°æ®é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    data_path = "data/uav.yaml"
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®é…ç½®æ–‡ä»¶ {data_path} ä¸å­˜åœ¨")
        sys.exit(1)

    print(f"âœ… æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"âœ… æ•°æ®é…ç½®: {data_path}")
    return model_path, data_path

def step1_baseline_validation(model_path, data_path):
    """æ­¥éª¤1: åŸºçº¿æ¨¡å‹éªŒè¯"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ­¥éª¤1: åŸºçº¿æ¨¡å‹éªŒè¯")
    print("=" * 60)

    try:
        model = YOLO(model_path)
        _ = model.val(
            data=data_path,
            imgsz=1088,
            batch=1,
            workers=0,  # è®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
            verbose=True,
            device=0
        )
        print("âœ… åŸºçº¿æ¨¡å‹éªŒè¯å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ åŸºçº¿æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        return False

def professional_speed_test(model_path, device='0', batch_size=1, img_size=1088,
                          warmup_runs=200, test_runs=1000, half_precision=False):
    """
    ä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯• - å‚è€ƒget_FPS.pyçš„æ ‡å‡†æ–¹æ³•

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡ ('0', 'cpu', etc.)
        batch_size: æ‰¹æ¬¡å¤§å°
        img_size: å›¾åƒå°ºå¯¸
        warmup_runs: é¢„çƒ­æ¬¡æ•°
        test_runs: æµ‹è¯•æ¬¡æ•°
        half_precision: æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦
    """
    print(f"\nâš¡ ä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯•")
    print(f"æ¨¡å‹: {model_path}")
    print(f"è®¾å¤‡: {device}")
    print(f"é¢„çƒ­: {warmup_runs}æ¬¡, æµ‹è¯•: {test_runs}æ¬¡")

    # 1. è®¾å¤‡é€‰æ‹©
    device = select_device(device, batch=batch_size)

    # 2. åŠ è½½æ¨¡å‹
    if model_path.endswith('.pt'):
        from ultralytics.nn.tasks import attempt_load_weights
        model = attempt_load_weights(model_path, device=device, fuse=True)
    elif model_path.endswith('.torchscript'):
        model = torch.jit.load(model_path, map_location=device)
    elif model_path.endswith('.onnx'):
        return test_onnx_speed(model_path, device, batch_size, img_size,
                              warmup_runs, test_runs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {model_path}")

    model = model.to(device)
    model.eval()

    # 3. åˆ›å»ºæµ‹è¯•è¾“å…¥
    if isinstance(img_size, int):
        input_shape = (batch_size, 3, img_size, img_size)
    else:
        input_shape = (batch_size, 3, *img_size)

    example_inputs = torch.randn(input_shape).to(device)

    # 4. åŠç²¾åº¦è®¾ç½®
    if half_precision and device.type == 'cuda':
        model = model.half()
        example_inputs = example_inputs.half()

    # 5. é¢„çƒ­é˜¶æ®µ
    print(f"ğŸ”¥ é¢„çƒ­é˜¶æ®µ...")
    with torch.no_grad():
        for i in tqdm(range(warmup_runs), desc='é¢„çƒ­'):
            _ = model(example_inputs)
            if device.type == 'cuda':
                torch.cuda.synchronize()

    # 6. æ­£å¼æµ‹è¯•
    print(f"â±ï¸  æ­£å¼æµ‹è¯•...")
    time_arr = []

    with torch.no_grad():
        for i in tqdm(range(test_runs), desc='æµ‹è¯•'):
            if device.type == 'cuda':
                torch.cuda.synchronize()

            start_time = time.time()
            _ = model(example_inputs)

            if device.type == 'cuda':
                torch.cuda.synchronize()

            end_time = time.time()
            time_arr.append(end_time - start_time)

    # 7. ç»Ÿè®¡åˆ†æ
    time_arr = np.array(time_arr)
    mean_time = np.mean(time_arr)
    std_time = np.std(time_arr)
    min_time = np.min(time_arr)
    max_time = np.max(time_arr)

    # æ¯å¼ å›¾ç‰‡çš„æ¨ç†æ—¶é—´
    infer_time_per_image = mean_time / batch_size
    fps = 1.0 / infer_time_per_image

    # 8. ç»“æœæŠ¥å‘Š
    print(f"\nğŸ“Š é€Ÿåº¦æµ‹è¯•ç»“æœ:")
    print(f"æ¨¡å‹å¤§å°: {get_weight_size(model_path)} MB")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {mean_time*1000:.2f} Â± {std_time*1000:.2f} ms")
    print(f"æ¯å¼ å›¾ç‰‡æ¨ç†æ—¶é—´: {infer_time_per_image*1000:.2f} ms")
    print(f"æ¨ç†é€Ÿåº¦ (FPS): {fps:.1f}")

    return {
        'mean_time': mean_time,
        'std_time': std_time,
        'fps': fps,
        'model_size': get_weight_size(model_path)
    }

def test_onnx_speed(model_path, device, batch_size, img_size, warmup_runs, test_runs):
    """ONNXæ¨¡å‹é€Ÿåº¦æµ‹è¯•"""
    try:
        import onnxruntime as ort
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… onnxruntime")
        return None

    print(f"ï¿½ æµ‹è¯•ONNXæ¨¡å‹: {model_path}")

    # è®¾ç½®ONNX Runtimeæä¾›è€… - ä¼˜å…ˆä½¿ç”¨GPU
    if device == 'cpu':
        providers = ['CPUExecutionProvider']
        print("âœ… ä½¿ç”¨CPUæ‰§è¡Œæä¾›è€…")
    elif torch.cuda.is_available():
        # æ£€æŸ¥CUDAæ‰§è¡Œæä¾›è€…æ˜¯å¦å¯ç”¨
        try:
            available_providers = ort.get_available_providers()
        except AttributeError:
            # æ—§ç‰ˆæœ¬å…¼å®¹æ€§
            available_providers = ['CPUExecutionProvider']

        if 'CUDAExecutionProvider' in available_providers:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            print("âœ… ä½¿ç”¨CUDAæ‰§è¡Œæä¾›è€… (GPUåŠ é€Ÿ)")
        else:
            providers = ['CPUExecutionProvider']
            print("âš ï¸  CUDAæ‰§è¡Œæä¾›è€…ä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:
        providers = ['CPUExecutionProvider']
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ‰§è¡Œæä¾›è€…")

    # åˆ›å»ºæ¨ç†ä¼šè¯
    session = ort.InferenceSession(model_path, providers=providers)

    # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
    input_name = session.get_inputs()[0].name

    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    if isinstance(img_size, int):
        input_shape = (batch_size, 3, img_size, img_size)
    else:
        input_shape = (batch_size, 3, *img_size)

    example_inputs = np.random.randn(*input_shape).astype(np.float32)

    # é¢„çƒ­
    print(f"ğŸ”¥ ONNXé¢„çƒ­...")
    for _ in tqdm(range(warmup_runs), desc='é¢„çƒ­'):
        _ = session.run(None, {input_name: example_inputs})

    # æµ‹è¯•
    print(f"â±ï¸  ONNXæµ‹è¯•...")
    time_arr = []

    for _ in tqdm(range(test_runs), desc='æµ‹è¯•'):
        start_time = time.time()
        _ = session.run(None, {input_name: example_inputs})
        end_time = time.time()
        time_arr.append(end_time - start_time)

    # ç»Ÿè®¡
    time_arr = np.array(time_arr)
    mean_time = np.mean(time_arr)
    std_time = np.std(time_arr)
    infer_time_per_image = mean_time / batch_size
    fps = 1.0 / infer_time_per_image

    print(f"\nğŸ“Š ONNXæµ‹è¯•ç»“æœ:")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {mean_time*1000:.2f} Â± {std_time*1000:.2f} ms")
    print(f"æ¯å¼ å›¾ç‰‡æ¨ç†æ—¶é—´: {infer_time_per_image*1000:.2f} ms")
    print(f"æ¨ç†é€Ÿåº¦ (FPS): {fps:.1f}")

    return {
        'mean_time': mean_time,
        'std_time': std_time,
        'fps': fps
    }

def step2_torchscript_benchmark(model_path, data_path):
    """æ­¥éª¤2: TorchScriptæ ¼å¼åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("ğŸš€ æ­¥éª¤2: TorchScriptæ ¼å¼åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    try:
        # æ‰‹åŠ¨å¯¼å‡ºå’Œæµ‹è¯•TorchScript
        model = YOLO(model_path)

        # å¯¼å‡ºTorchScript (åœ¨GPUä¸Š)
        print("ğŸ”„ å¯¼å‡ºTorchScriptæ ¼å¼...")
        model.to('cuda:0')  # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
        ts_path = model.export(format='torchscript', imgsz=1088, device='cuda:0')

        # æµ‹è¯•TorchScriptæ¨¡å‹
        print("ğŸ”„ æµ‹è¯•TorchScriptæ¨¡å‹...")
        ts_model = YOLO(ts_path)
        _ = ts_model.val(data=data_path, imgsz=1088, batch=1, workers=0, verbose=True)

        print("âœ… TorchScriptæµ‹è¯•å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ TorchScriptæµ‹è¯•å¤±è´¥: {e}")
        return False

def step3_onnx_benchmark(model_path, data_path):
    """æ­¥éª¤3: ONNXæ ¼å¼åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("ğŸ”§ æ­¥éª¤3: ONNXæ ¼å¼åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    try:
        # æ‰‹åŠ¨å¯¼å‡ºå’Œæµ‹è¯•ONNX
        model = YOLO(model_path)

        # å¯¼å‡ºONNX (åœ¨GPUä¸Š)
        print("ğŸ”„ å¯¼å‡ºONNXæ ¼å¼...")
        model.to('cuda:0')  # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
        onnx_path = model.export(format='onnx', imgsz=1088, device='cuda:0')

        # é¦–å…ˆå°è¯•GPUæµ‹è¯•ï¼Œå¤±è´¥åˆ™å›é€€åˆ°CPU
        print("ğŸ”„ æµ‹è¯•ONNXæ¨¡å‹...")
        onnx_model = YOLO(onnx_path)

        # å°è¯•GPUæµ‹è¯•
        try:
            print("ğŸš€ å°è¯•åœ¨GPUä¸Šæµ‹è¯•ONNX...")
            _ = onnx_model.val(
                data=data_path,
                imgsz=1088,
                batch=1,
                workers=0,
                device=0,  # å°è¯•ä½¿ç”¨GPU
                verbose=True
            )
            print("âœ… ONNX GPUæµ‹è¯•æˆåŠŸ")
        except Exception as gpu_e:
            print(f"âš ï¸  GPUæµ‹è¯•å¤±è´¥: {gpu_e}")
            print("ğŸ”„ å›é€€åˆ°CPUæµ‹è¯•...")
            _ = onnx_model.val(
                data=data_path,
                imgsz=1088,
                batch=1,
                workers=0,
                device='cpu',  # å›é€€åˆ°CPU
                verbose=True
            )
            print("âœ… ONNX CPUæµ‹è¯•æˆåŠŸ")

        print("âœ… ONNXåŸºå‡†æµ‹è¯•å®Œæˆ")
        return True
    except Exception as e:
        print(f"âš ï¸  ONNXåŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ’¡ ONNXæµ‹è¯•è·³è¿‡ï¼Œè¿™é€šå¸¸æ˜¯ç”±äºCUDAåº“ç‰ˆæœ¬é—®é¢˜")
        return False

def step4_professional_speed_test(model_path):
    """æ­¥éª¤4: ä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("âš¡ æ­¥éª¤4: ä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯•")
    print("=" * 60)

    formats = [
        ('last.pt', 'PyTorch'),
        ('last.torchscript', 'TorchScript'),
        ('last.onnx', 'ONNX')
    ]

    results = {}

    for model_file, format_name in formats:
        if Path(model_file).exists():
            print(f"\nğŸ§ª æµ‹è¯• {format_name} æ ¼å¼...")
            try:
                # å¯¹äºONNXï¼Œä½¿ç”¨GPUè®¾å¤‡è¿›è¡Œæµ‹è¯•
                device = '0' if model_file.endswith('.onnx') else '0'
                result = professional_speed_test(
                    model_path=model_file,
                    device=device,
                    batch_size=1,
                    img_size=1088,
                    warmup_runs=100,  # å‡å°‘æµ‹è¯•æ—¶é—´ä½†ä¿æŒå‡†ç¡®æ€§
                    test_runs=500,
                    half_precision=False
                )
                results[format_name] = result
            except Exception as e:
                print(f"âŒ {format_name} æµ‹è¯•å¤±è´¥: {e}")
                results[format_name] = None
        else:
            print(f"âš ï¸  {format_name} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")

    # æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
    if results:
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š:")
        print(f"=" * 80)
        print(f"{'æ ¼å¼':<12} {'æ¨ç†æ—¶é—´(ms)':<15} {'FPS':<10} {'ç›¸å¯¹æ€§èƒ½':<10} {'æ¨¡å‹å¤§å°(MB)':<12}")
        print(f"-" * 80)

        baseline_fps = None
        for format_name, result in results.items():
            if result:
                fps = result['fps']
                inference_time = result['mean_time'] * 1000
                model_size = result.get('model_size', 'N/A')

                if baseline_fps is None:
                    baseline_fps = fps
                    relative_perf = "åŸºå‡†"
                else:
                    relative_perf = f"{fps/baseline_fps:.2f}x"

                print(f"{format_name:<12} {inference_time:<15.2f} {fps:<10.1f} {relative_perf:<10} {model_size:<12}")
            else:
                print(f"{format_name:<12} {'å¤±è´¥':<15} {'N/A':<10} {'N/A':<10} {'N/A':<12}")
        print(f"=" * 80)

    return results

def step4_speed_test(model_path):
    """æ­¥éª¤4: å®é™…æ¨ç†é€Ÿåº¦æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("âš¡ æ­¥éª¤4: å®é™…æ¨ç†é€Ÿåº¦æµ‹è¯•")
    print("=" * 60)

    try:
        import glob
        model = YOLO(model_path)
        test_images = glob.glob("data/images/*.jpg")[:10]  # å–å‰10å¼ å›¾ç‰‡æµ‹è¯•

        if not test_images:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
            return False

        print(f"ğŸ”„ ä½¿ç”¨ {len(test_images)} å¼ å›¾ç‰‡è¿›è¡Œé€Ÿåº¦æµ‹è¯•...")

        # é¢„çƒ­
        print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        for _ in range(3):
            model.predict(test_images[0], imgsz=1088, verbose=False)

        # æ­£å¼æµ‹è¯•
        print("â±ï¸  å¼€å§‹é€Ÿåº¦æµ‹è¯•...")
        start_time = time.time()
        for img_path in test_images:
            _ = model.predict(img_path, imgsz=1088, verbose=False)
        end_time = time.time()

        avg_time = (end_time - start_time) / len(test_images)
        fps = 1.0 / avg_time

        print(f"ğŸ“Š å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.1f}ms")
        print(f"ğŸ“Š æ¨ç†é€Ÿåº¦: {fps:.1f} FPS")
        print("âœ… é€Ÿåº¦æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ é€Ÿåº¦æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_complete_benchmark():
    """è¿è¡Œå®Œæ•´çš„æ¨¡å‹åŸºå‡†æµ‹è¯•"""
    print("ğŸ¯ å¼€å§‹æ ‡å‡†æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    print("=" * 60)

    # è®¾ç½®ç¯å¢ƒ
    model_path, data_path = setup_environment()

    # è®°å½•æµ‹è¯•ç»“æœ
    results = {
        'baseline': False,
        'torchscript': False,
        'onnx': False,
        'speed': False
    }

    # æ­¥éª¤1: åŸºçº¿éªŒè¯
    results['baseline'] = step1_baseline_validation(model_path, data_path)

    # æ­¥éª¤2: TorchScriptæµ‹è¯•
    results['torchscript'] = step2_torchscript_benchmark(model_path, data_path)

    # æ­¥éª¤3: ONNXæµ‹è¯•
    results['onnx'] = step3_onnx_benchmark(model_path, data_path)

    # æ­¥éª¤4: ä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯•
    speed_results = step4_professional_speed_test(model_path)
    results['speed'] = bool(speed_results)

    # æ€»ç»“ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    for test_name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"{test_name.upper():<12}: {status}")

    successful_tests = sum(results.values())
    total_tests = len(results)
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {successful_tests}/{total_tests} é¡¹æµ‹è¯•æˆåŠŸ")

    if successful_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œä½†æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ ‡å‡†æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--weights', type=str, default='last.pt', help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--device', default='0', help='è®¾å¤‡: 0, cpuç­‰')
    parser.add_argument('--batch', type=int, default=1, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--imgsz', type=int, default=1088, help='å›¾åƒå°ºå¯¸')
    parser.add_argument('--warmup', type=int, default=200, help='é¢„çƒ­æ¬¡æ•°')
    parser.add_argument('--runs', type=int, default=1000, help='æµ‹è¯•æ¬¡æ•°')
    parser.add_argument('--half', action='store_true', help='åŠç²¾åº¦æ¨¡å¼')
    parser.add_argument('--speed-only', action='store_true', help='ä»…è¿›è¡Œé€Ÿåº¦æµ‹è¯•')

    args = parser.parse_args()

    if args.speed_only:
        # ä»…è¿›è¡Œä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯•
        print("ğŸ¯ ä¸“ä¸šçº§é€Ÿåº¦æµ‹è¯•æ¨¡å¼")
        professional_speed_test(
            model_path=args.weights,
            device=args.device,
            batch_size=args.batch,
            img_size=args.imgsz,
            warmup_runs=args.warmup,
            test_runs=args.runs,
            half_precision=args.half
        )
    else:
        # å®Œæ•´åŸºå‡†æµ‹è¯•
        run_complete_benchmark()
    
